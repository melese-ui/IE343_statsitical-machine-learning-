{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 (Full mark: 100pt)\n",
    "- Questions 1~3: Conceptual\n",
    "- Questions 4~8: Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision tree (15pt)\n",
    "\n",
    "**It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form**\n",
    "\n",
    "$f(X)=\\sum_{j=1}^{p} f_{j}\\left(X_{j}\\right)$\n",
    "\n",
    "**Explain why this is the case. You can begin with (8.12) in Algorithm 8.2 in the textbook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperplanes (8pt)\n",
    "\n",
    "**We have seen that in $p = 2$ dimensions, a linear decision boundary takes the form $\\beta_0+\\beta_1X_1+\\beta_2X_2 = 0$. We now investigate a non-linear decision boundary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Sketch the curve**: $\\left(1+X_{1}\\right)^{2}+\\left(2-X_{2}\\right)^{2}=4$\n",
    "\n",
    "**You can create plots using a Python package like matplotlib, or you can draw them by hand.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) On your sketch, indicate the set of points for which** $\\left(1+X_{1}\\right)^{2}+\\left(2-X_{2}\\right)^{2}>4$ **as well as the set of points for which** $\\left(1+X_{1}\\right)^{2}+\\left(2-X_{2}\\right)^{2}\\leq4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Suppose that a classifier assigns an observation to the blue class if** $\\left(1+X_{1}\\right)^{2}+\\left(2-X_{2}\\right)^{2}>4$**, and to the red class otherwise. To what class is the observation (0, 0) classified? (âˆ’1, 1)? (2, 2)? (3, 8)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Argue that while the decision boundary in (c) is not linear in terms of $X_{1}$ and $X_{2}$, it is linear in terms of $X_{1}, X_{1}^{2}, X_{2}$, and $X_{2}^{2}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hierarchical clustering (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose that for a particular data set, we perform hierarchical clustering using single linkage and using complete linkage. We obtain two dendrograms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) At a certain point on the single linkage dendrogram, the clusters $\\{1,2,3\\}$ and $\\{4,5\\}$ fuse. On the complete linkage dendrogram, the clusters $\\{1,2,3\\}$ and $\\{4,5\\}$ also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) At a certain point on the single linkage dendrogram, the clusters $\\{5\\}$ and $\\{6\\}$ fuse. On the complete linkage dendrogram, the clusers $\\{5\\}$ and $\\{6\\}$ also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or s there not enough information to tell?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bagging (10pt)\n",
    "\n",
    "**In Lab 7, we implemented bagged decision tree using** ````RandomForestRegressor(max_features=len(X_train.columns))```` **since random forest that uses all the features is equivalent to bagging. For this question, implement bagging  by using  ````DecisionTreeRegressor````**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "boston_df = pd.read_csv('datda/Boston.csv').drop('Unnamed: 0', axis=1).dropna()\n",
    "X = boston_df.drop('medv', axis=1)\n",
    "y = boston_df.medv\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=0)\n",
    "\n",
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SVM 1 (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions.**\n",
    "\n",
    "**You can use Scikit learn library for SVM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SVM 2 (16pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/OJ.csv', index_col=0)\n",
    "# Define predictors and response \n",
    "X = df.drop(axis=1, labels=['Purchase'])\n",
    "y = df['Purchase']\n",
    "# Dummy variables to transform qualitative into quantitative variables\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Fit a support vector classifier (```SVC```) to the training data using ````cost=0.01````, with ````Purchase```` as the response and the other variables as predictors (````cost```` refers to the regularization coefficient).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) What are the training and test error rates? Use confusion matrix for this problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Use the ````GridSearchCV```` function to select an optimal ````cost````. Consider values in the range $0.01$ to $10$. Do as much as you PC allows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compute the training and test error rates using this new value for ````cost````.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for ````gamma````.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set ````degree=2````.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Overall, which approach seems to give the best results on this data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clustering (16pt)\n",
    "\n",
    "**In Lab 9, we implemented K-means clustering using ````KMeans()```` . For this question, you will implement K-means clustering from scratch following the class format of sklearn. Reproduce Section 2.1 in Lab 9. Note that you may not be able to get the exact same results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(2)\n",
    "X = np.random.standard_normal((25,2))\n",
    "X[:25,0] = X[:25,0]+3\n",
    "X[:25,1] = X[:25,1]-4\n",
    "\n",
    "# KMeans class\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters=8, n_init=10, max_iter=300, tol=0.0001):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init        \n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "\n",
    "    def fit(self, data):\n",
    "        ## Write your answer here\n",
    "        \n",
    "        self.labels_ = # return the best labels\n",
    "        self.cluster_centers_ = # return the best cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running KMeans\n",
    "# km1 = KMeans(n_clusters=2, n_init=20)\n",
    "# km1.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. PCA: Proportion of Variance Explained (PVE) (15pt)\n",
    "\n",
    "**In Lab 9, we learned how to calculate PVE by using ``sklearn``. In this exercise, calculate PVE from scratch by implementing the following equation:**\n",
    "\n",
    "$\\frac{\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{p} \\phi_{j m} x_{i j}\\right)^{2}}{\\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{i j}^{2}}$ (Equation 10.8 in the textbook)\n",
    "\n",
    "**and compare the results with the PVE that is computed by ``sklearn``.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Import dataset\n",
    "df = pd.read_csv('data/USArrests.csv', index_col=0)\n",
    "X = pd.DataFrame(scale(df), index=df.index, columns=df.columns)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your answer here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
